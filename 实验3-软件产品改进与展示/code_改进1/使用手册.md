## scrapy项目部分改进使用说明
### 改进项说明
1. 基于mongodb分布式文档数据库将scrapy改成分布式应用。
2. 基于布隆过滤器和mongodb文档实现url的两级过滤器。
3. 记录爬取网站的链接结构，进行网站画像。
> 针对这部分的改进，本文件夹下负责网站链接信息的记录，具体画像实现请参考其他改进部分文件夹下的说明。

### 使用说明
scrapy是Python的爬虫框架，以第三方的Python类库形式提供使用。在安装过程中会进行一些其他设置，包括添加scrapy的环境变量等。所以建议先下载安装scrapy，然后依据本文档说明进行设置修改。

python 关于布隆过滤器的第三方实现由三个：pybloom、pybloomfilter、pybloomfiltermmap。其中pybloom只能用于Windows平台，pybloomfiltermmap只能用于Linux平台，pybloomfilter windows和Linux都有使用问题。由于没有一个跨平台的第三方包可以使用，所以我们用Python内置的数据结构bitarray实现了一个布隆过滤器。如果你是在linux平台上做开发，建议你安装pybloomfiltermmap作为布隆过滤器来使用。

关于Linux平台使用pybloomfiltermmap作为布隆过滤器时，仅需要修改dupefilters.py中对于布隆过滤器的引用即可，您可以自行进行修改，如果有任何修改问题，可以联系我们。

1. 将filters.py、mongodb_agent.py、dupefilters.py文件放到scrapy根目录下。
2. 将engine.py、scheduler.py文件放在scrapy/core/目录下。
3. 将__init__.py文件放在scrapy/http/request/目录下。
4. 将default_settings.py文件放在scrapy/settings目录下。

### 开发者阅读
由于这部分的改进针对scrapy框架本身进行了修改，在使用方式上尽可能保持原有scrapy的使用方式，但是其中有些地方为了支持新实现的功能还是做出了修改，现作一说明：

1. settings部分增加了新的配置项。爬虫开发者可以在自己开发的爬虫项目中直接使用，使用方式和常规的scrapy配置项相同。
> |名称|类型|说明|
> |---:|:---:|:---:|
> |DB_HOST|str|mongodb数据库服务器ip地址|
> |DB_PORT|str|mongodb数据库服务器端口地址|
> |DB_NAME|str|mongodb数据库名称|
> |DB_COLLECTIONS_NAME|str|mongodb数据库集合名称|
> |MAX_LENGTH|int|预计的URL个数|
> |ERROR_RATE|float|允许的错误率|

2. 为进行网站结构的画像，需要爬虫开发者在开发的parse函数中生成新的Request的部分添加一个初始化参数，使用方式与未修改的 scrapy parse函数的使用方式一样。现给出一个使用示例。

```python
from scrapy import Spider,Request

class YouSpider(Spider):
    name = "myspider"
    start_urls = [
        'http://www.mafengwo.cn/i/6579578.html']

    def parse(self, response):
        source = response.url    # 通过response.url获取该页面的url

        # 本示例直接使用固定的url生成新的Request，通常应该用xpath、re、cssselector等工具抽取。
        
        # 如果不想进行网站画像，则与通常scrapy使用方式相同，直接使用
        # return Request("http://www.sohu.com")
        # 否则，可通过source_url命名参数指定新生成的Request的源url。
        return Request("http://www.sohu.com/", source_url=source)    
        

```

****
*备注：欢迎提交使用过程中出现的bug*

